{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5bc7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5163a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_tmm_normal(n_layers, d_layers, wavelengths):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of the Transfer Matrix Method for normal incidence.\n",
    "    Allows backpropagation through refractive indices (n_layers) and thicknesses (d_layers).\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    num_wavelengths = wavelengths.shape[0]\n",
    "    num_layers = n_layers.shape[0]\n",
    "\n",
    "    # Ensure inputs are complex for TMM calculations\n",
    "    n_layers_c = n_layers.unsqueeze(0).expand(num_wavelengths, -1).cfloat()\n",
    "    d_layers_c = d_layers.unsqueeze(0).expand(num_wavelengths, -1).cfloat()\n",
    "    wl_c = wavelengths.unsqueeze(1).cfloat()\n",
    "\n",
    "    # Wave vector in each layer: k = 2 * pi * n / lambda\n",
    "    k = 2 * np.pi * n_layers_c / wl_c\n",
    "\n",
    "    # Phase shift for each layer: phi = k * d\n",
    "    # Note: First and last layers are semi-infinite air/substrate, so d=0 effectively for phase,\n",
    "    # but we handle them as boundaries below.\n",
    "    phi = k * d_layers_c\n",
    "\n",
    "    # 2x2 Transfer matrices for each layer j\n",
    "    # M_j = [[cos(phi), (-i/n)*sin(phi)], [-i*n*sin(phi), cos(phi)]]\n",
    "    cos_phi = torch.cos(phi)\n",
    "    sin_phi = torch.sin(phi)\n",
    "    zeros = torch.zeros_like(cos_phi)\n",
    "    ones = torch.ones_like(cos_phi)\n",
    "\n",
    "    # Stack components into (Num_WL, Num_Layers, 2, 2) tensor\n",
    "    M = torch.stack([\n",
    "        torch.stack([cos_phi, -1j / n_layers_c * sin_phi], dim=-1),\n",
    "        torch.stack([-1j * n_layers_c * sin_phi, cos_phi], dim=-1)\n",
    "    ], dim=-2)\n",
    "\n",
    "    # Compute total transfer matrix by multiplying all layer matrices\n",
    "    # Start with Identity matrix\n",
    "    M_total = torch.eye(2, dtype=torch.cfloat, device=DEVICE).unsqueeze(0).repeat(num_wavelengths, 1, 1)\n",
    "\n",
    "    # Loop through standard layers (excluding semi-infinite substrate/air if handled separately,\n",
    "    # here we assume d=0 for first/last if they are just boundaries).\n",
    "    # For a standard stack: Air -> L1 -> L2 ... -> LN -> Substrate\n",
    "    for i in range(num_layers):\n",
    "         M_total = torch.matmul(M_total, M[:, i, :, :])\n",
    "\n",
    "    # Calculate Transmission Coefficient t\n",
    "    # t = 2 * n0 / (M00 + M01*ns + n0*M10 + n0*M11*ns)\n",
    "    # Assuming n0 (air) is index 0, and ns (substrate) is last index.\n",
    "    n0 = n_layers_c[:, 0]\n",
    "    ns = n_layers_c[:, -1]\n",
    "\n",
    "    m00 = M_total[:, 0, 0]\n",
    "    m01 = M_total[:, 0, 1]\n",
    "    m10 = M_total[:, 1, 0]\n",
    "    m11 = M_total[:, 1, 1]\n",
    "\n",
    "    t = (2 * n0) / (m00 + m01 * ns + n0 * m10 + n0 * m11 * ns)\n",
    "\n",
    "    # Transmittance T = |t|^2 * (ns / n0)\n",
    "    T = torch.abs(t)**2 * (torch.real(ns) / torch.real(n0))\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886cb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineOptimizer(nn.Module):\n",
    "    def __init__(self, num_layers_to_optimize, seed_size=100):\n",
    "        super(OnlineOptimizer, self).__init__()\n",
    "        # Simple MLP architecture as suggested by general deep learning practices in standard inverse design\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(seed_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_layers_to_optimize),\n",
    "            nn.Sigmoid() # Sigmoid to bound output between 0 and 1 for easy scaling\n",
    "        )\n",
    "\n",
    "    def forward(self, seed):\n",
    "        # Output is in [0, 1]. We scale this to the desired refractive index range.\n",
    "        # The text specifies a range of [1.6, 2.4] for SiNx[cite: 1188].\n",
    "        n_min = 1.6\n",
    "        n_max = 2.4\n",
    "        out = self.net(seed)\n",
    "        n_optimized = out * (n_max - n_min) + n_min\n",
    "        return n_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d713ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_systematic_error(n_layers, error_type='linear_gradient'):\n",
    "    \"\"\"Applies deterministic errors as described in Section 3.4.3\"\"\"\n",
    "    num_active_layers = n_layers.shape[0]\n",
    "    device = n_layers.device\n",
    "\n",
    "    if error_type == 'linear_gradient':\n",
    "        # Simulate a linear drift in refractive index from bottom to top [cite: 1370-1371]\n",
    "        # Max drift 0.4 as per text example [cite: 1371]\n",
    "        drift = torch.linspace(0, 0.4, steps=num_active_layers, device=device)\n",
    "        return n_layers + drift\n",
    "\n",
    "    # Add other types (sinusoidal, transition regions) here as needed\n",
    "    return n_layers\n",
    "\n",
    "def apply_random_error(n_layers, gamma=0.01, mu=0.0):\n",
    "    \"\"\"\n",
    "    Applies random additive noise as described in Equation 3.6[cite: 1514].\n",
    "    n_imp = n_id + n_id * n_rand\n",
    "    n_rand = 2 * gamma * U(0,1) - mu\n",
    "    \"\"\"\n",
    "    # Uniform noise U(0,1)\n",
    "    U = torch.rand_like(n_layers)\n",
    "    n_rand = 2 * gamma * U - mu\n",
    "    n_imperfect = n_layers + (n_layers * n_rand)\n",
    "    return n_imperfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95d1cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 1: Ideal Optimization...\n",
      "Epoch 0, Loss: 0.124598\n",
      "Epoch 100, Loss: 0.031449\n",
      "Epoch 200, Loss: 0.030015\n",
      "Epoch 300, Loss: 0.029473\n",
      "Epoch 400, Loss: 0.029348\n",
      "\n",
      "Starting Stage 2: Fabrication-in-the-loop Retraining (Linear Drift)...\n",
      "Epoch 600, Loss: 0.026823\n",
      "Epoch 700, Loss: 0.026763\n",
      "Epoch 800, Loss: 0.026730\n",
      "Optimization Complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Parameters ---\n",
    "NUM_LAYERS = 50  # Number of layers to optimize (excluding air/substrate)\n",
    "SEED_SIZE = NUM_LAYERS # Size of the random seed vector\n",
    "WAVELENGTHS = torch.linspace(400, 700, 300).to(DEVICE) # Visible spectrum 400-700nm [cite: 1097]\n",
    "TARGET_SPECTRUM = torch.ones_like(WAVELENGTHS)\n",
    "# Define a simple Bandstop target (e.g., reflect 530-570nm)\n",
    "TARGET_SPECTRUM[(WAVELENGTHS > 530) & (WAVELENGTHS < 570)] = 0.0\n",
    "\n",
    "# Fixed parameters\n",
    "N_AIR = torch.tensor([1.0]).to(DEVICE)\n",
    "N_SUBSTRATE = torch.tensor([1.5]).to(DEVICE) # Approx glass/SiO2\n",
    "LAYER_THICKNESS = 30e-9 # 30nm fixed thickness as in some examples [cite: 1186]\n",
    "\n",
    "# --- Initialization ---\n",
    "# The fixed seed that remains constant throughout optimization [cite: 1179]\n",
    "FIXED_SEED = torch.rand(SEED_SIZE).to(DEVICE)\n",
    "\n",
    "model = OnlineOptimizer(NUM_LAYERS, SEED_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Adam is standard [cite: 2088]\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --- Training Helper Function ---\n",
    "def train_step(stage_2=False, error_type=None, rand_gamma=0.0):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. NN generates \"ideal\" design from fixed seed\n",
    "    n_generated = model(FIXED_SEED)\n",
    "\n",
    "    # 2. Apply Fabrication Errors if in Stage 2 [cite: 1482]\n",
    "    if stage_2:\n",
    "        if error_type:\n",
    "             n_used = apply_systematic_error(n_generated, error_type)\n",
    "        else:\n",
    "             n_used = n_generated\n",
    "\n",
    "        if rand_gamma > 0:\n",
    "             n_used = apply_random_error(n_used, gamma=rand_gamma)\n",
    "    else:\n",
    "        n_used = n_generated\n",
    "\n",
    "    # Assemble full stack for TMM (Air + Active Layers + Substrate)\n",
    "    n_full_stack = torch.cat([N_AIR, n_used, N_SUBSTRATE])\n",
    "\n",
    "    # Create thickness tensor (0 for air/substrate to treat as infinite boundaries in this simple TMM)\n",
    "    d_active = torch.full((NUM_LAYERS,), LAYER_THICKNESS, device=DEVICE)\n",
    "    d_full_stack = torch.cat([torch.tensor([0.0], device=DEVICE), d_active, torch.tensor([0.0], device=DEVICE)])\n",
    "\n",
    "    # 3. Differentiable TMM Solver calculates spectrum\n",
    "    transmission = differentiable_tmm_normal(n_full_stack, d_full_stack, WAVELENGTHS * 1e-9)\n",
    "\n",
    "    # 4. Calculate Loss & Backpropagate\n",
    "    loss = criterion(transmission, TARGET_SPECTRUM)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), transmission.detach(), n_generated.detach()\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "# STAGE 1: Ideal Optimization [cite: 1184]\n",
    "print(\"Starting Stage 1: Ideal Optimization...\")\n",
    "for epoch in range(500):\n",
    "    loss_val, current_spectrum, current_design = train_step(stage_2=False)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_val:.6f}\")\n",
    "\n",
    "# STAGE 2: Fabrication-in-the-loop Retraining [cite: 1468]\n",
    "# We continue training the SAME model, but now with errors engaged.\n",
    "print(\"\\nStarting Stage 2: Fabrication-in-the-loop Retraining (Linear Drift)...\")\n",
    "for epoch in range(501, 801):\n",
    "    # Applying a linear gradient systematic error during training\n",
    "    loss_val, current_spectrum, final_design = train_step(stage_2=True, error_type='linear_gradient')\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_val:.6f}\")\n",
    "\n",
    "print(\"Optimization Complete.\")\n",
    "\n",
    "# Final check: The NN output 'final_design' is now \"pre-distorted\" to\n",
    "# compensate for the linear gradient error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eea58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chp3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
